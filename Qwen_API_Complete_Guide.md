# QWen å¤§æ¨¡å‹ API å®Œæ•´è°ƒç ”æŒ‡å—

## ç›®å½•
1. [å®˜æ–¹APIæ¥å£æ–‡æ¡£](#å®˜æ–¹apiæ¥å£æ–‡æ¡£)
2. [ä¸»è¦æ¥å£ç±»å‹](#ä¸»è¦æ¥å£ç±»å‹)
3. [Pythonè°ƒç”¨ç¤ºä¾‹](#pythonè°ƒç”¨ç¤ºä¾‹)
4. [æœ€ä½³å®è·µå’Œå»ºè®®](#æœ€ä½³å®è·µå’Œå»ºè®®)
5. [å¸¸è§é—®é¢˜è§£å†³](#å¸¸è§é—®é¢˜è§£å†³)

---

## å®˜æ–¹APIæ¥å£æ–‡æ¡£

### 1.1 åŸºç¡€ä¿¡æ¯

**å®˜æ–¹ç½‘å€**: https://dashscope.aliyun.com/

Qwenæ˜¯ç”±é˜¿é‡Œå·´å·´é›†å›¢æ¨å‡ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæä¾›é€šè¿‡DashScopeå¹³å°çš„APIæœåŠ¡ã€‚

### 1.2 APIè°ƒç”¨æ–¹å¼

Qwenæ”¯æŒä¸¤ç§ä¸»æµAPIè°ƒç”¨æ–¹å¼ï¼š

#### æ–¹å¼ä¸€ï¼šOpenAIå…¼å®¹æ¥å£ï¼ˆæ¨èç”¨äºå¿«é€Ÿè¿ç§»ï¼‰
```
Base URL: https://dashscope.aliyun.com/compatible-mode/v1
```

ä¼˜ç‚¹ï¼š
- ä¸OpenAI APIå…¼å®¹
- å¯ç›´æ¥ä½¿ç”¨OpenAI SDK
- è¿ç§»æˆæœ¬ä½

#### æ–¹å¼äºŒï¼šDashScopeåŸç”Ÿæ¥å£
```
Base URL: https://dashscope.aliyun.com/api/v1
```

ä¼˜ç‚¹ï¼š
- æ›´å¤šé«˜çº§åŠŸèƒ½æ”¯æŒ
- æ›´è¯¦ç»†çš„å“åº”ä¿¡æ¯
- æ›´çµæ´»çš„é…ç½®é€‰é¡¹

### 1.3 é‰´æƒæ–¹å¼

**API Keyè·å–æ­¥éª¤**ï¼š
1. è®¿é—® https://dashscope.aliyun.com/
2. ç™»å½•é˜¿é‡Œäº‘è´¦æˆ·
3. åˆ›å»ºAPI Key
4. å¤åˆ¶ä¿å­˜åˆ°ç¯å¢ƒå˜é‡ï¼š`DASHSCOPE_API_KEY`

**é‰´æƒå¤´ä¿¡æ¯**ï¼š
```
Authorization: Bearer YOUR_API_KEY
```

### 1.4 æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨

#### ğŸ“± é€šç”¨å¯¹è¯æ¨¡å‹ï¼ˆæ¨èï¼‰
| æ¨¡å‹åç§° | ç‰¹ç‚¹ | ç”¨é€” | è¾“å…¥ä»·æ ¼(Â¥/1K) | è¾“å‡ºä»·æ ¼(Â¥/1K) |
|---------|------|------|---------------|---------------|
| qwen-turbo | å¿«é€Ÿã€ç»æµ | é€šç”¨å¯¹è¯ã€ç®€å•ä»»åŠ¡ | **0.0003** | **0.0006** |
| qwen-plus | å¹³è¡¡æ€§å¼º | **æ¨èç”¨äºå¤§å¤šæ•°åœºæ™¯** | **0.0008** | **0.002** |
| qwen-max | æ€§èƒ½æœ€å¼º | å¤æ‚æ¨ç†ã€ä¸“ä¸šä»»åŠ¡ | **0.02** | **0.06** |
| qwen-max-latest | æœ€æ–°ç‰ˆæœ¬ | æœ€æ–°åŠŸèƒ½æ”¯æŒ | 0.02 | 0.06 |

#### ğŸš€ è¶…å¤§è§„æ¨¡æ¨¡å‹
| æ¨¡å‹åç§° | å‚æ•°é‡ | ç‰¹ç‚¹ |
|---------|-------|------|
| qwen-ultra | å¯å˜å‚æ•° | æ€§èƒ½æœ€å¼ºï¼Œæˆæœ¬æœ€é«˜ |
| qwen-long | å¯å˜å‚æ•° | æ”¯æŒè¶…é•¿ä¸Šä¸‹æ–‡(100K tokens) |

#### ğŸ’» ä»£ç å’Œç¼–ç¨‹æ¨¡å‹
| æ¨¡å‹åç§° | ç‰¹ç‚¹ | ç”¨é€” |
|---------|------|------|
| qwen-coder | ä»£ç ä¼˜åŒ– | ä»£ç ç”Ÿæˆã€è°ƒè¯•ã€åˆ†æ |
| qwen-math | æ•°å­¦ä¼˜åŒ– | æ•°å­¦é—®é¢˜æ±‚è§£ |

#### ğŸ¨ å¤šæ¨¡æ€æ¨¡å‹
| æ¨¡å‹åç§° | æ”¯æŒç±»å‹ | ç”¨é€” |
|---------|--------|------|
| qwen-vl-plus | æ–‡æœ¬+å›¾åƒ | å›¾åƒç†è§£ã€OCR |
| qwen-vl-max | æ–‡æœ¬+é«˜æ¸…å›¾åƒ | é«˜ç²¾åº¦å›¾åƒåˆ†æ |
| qwen-audio | æ–‡æœ¬+è¯­éŸ³ | è¯­éŸ³è¯†åˆ«ã€è½¬å½• |

### 1.5 åœ°åŸŸç«¯ç‚¹

```
å…¨çƒåœ°åŸŸï¼šhttps://dashscope.aliyun.com/api/v1
æ–°åŠ å¡ï¼š   https://dashscope-sg.aliyuncs.com/api/v1
æ—¥æœ¬ï¼š     https://dashscope-jp.aliyuncs.com/api/v1
```

### 1.6 è¯·æ±‚/å“åº”æ ¼å¼

#### è¯·æ±‚æ ¼å¼ç¤ºä¾‹
```json
{
  "model": "qwen-plus",
  "messages": [
    {
      "role": "user",
      "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚è¯·å›ç­”é—®é¢˜ã€‚"
    }
  ],
  "temperature": 0.7,
  "top_p": 0.8,
  "max_tokens": 2048
}
```

#### å“åº”æ ¼å¼ç¤ºä¾‹
```json
{
  "status_code": 200,
  "request_id": "uuid-string",
  "code": null,
  "message": null,
  "output": {
    "choices": [
      {
        "finish_reason": "stop",
        "message": {
          "role": "assistant",
          "content": "å›å¤å†…å®¹"
        }
      }
    ],
    "usage": {
      "input_tokens": 10,
      "output_tokens": 50
    }
  }
}
```

---

## ä¸»è¦æ¥å£ç±»å‹

### 2.1 æ–‡æœ¬ç”Ÿæˆ/å¯¹è¯æ¥å£

**ç«¯ç‚¹**: `POST /v1/services/aigc/text-generation/generation`

**å‚æ•°è¯´æ˜**:
- `model`: æ¨¡å‹åç§°
- `messages`: å¯¹è¯å†å²ï¼ŒåŒ…å«roleå’Œcontent
- `temperature`: 0-2ï¼Œæ§åˆ¶éšæœºæ€§ï¼ˆé»˜è®¤0.7ï¼‰
- `top_p`: 0-1ï¼Œæ ¸é‡‡æ ·å‚æ•°ï¼ˆé»˜è®¤0.8ï¼‰
- `max_tokens`: æœ€å¤§è¾“å‡ºtokenæ•°
- `stream`: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º

**å…³é”®å‚æ•°è¯¦è§£**ï¼š

| å‚æ•° | èŒƒå›´ | é»˜è®¤å€¼ | è¯´æ˜ |
|-----|------|-------|------|
| temperature | 0-2 | 0.7 | å€¼è¶Šå°è¾“å‡ºè¶Šç¡®å®šæ€§ï¼Œå€¼è¶Šå¤§è¶Šéšæœº |
| top_p | 0-1 | 0.8 | æ ¸é‡‡æ ·ï¼Œé€‰æ‹©ç´¯ç§¯æ¦‚ç‡æœ€é«˜çš„token |
| top_k | 1-50 | æ—  | é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„Kä¸ªtoken |
| max_tokens | 1-æœ€å¤§å€¼ | æ— é™åˆ¶ | é™åˆ¶è¾“å‡ºé•¿åº¦ï¼ŒèŠ‚çœæˆæœ¬ |
| repetition_penalty | 0-2 | 1.0 | æ§åˆ¶é‡å¤å†…å®¹ï¼Œ>1å‡å°‘é‡å¤ |

### 2.2 æµå¼å“åº”æ¥å£

**ç‰¹ç‚¹**ï¼š
- å®æ—¶è¿”å›æ•°æ®ï¼Œæ— éœ€ç­‰å¾…å®Œæ•´å“åº”
- æ”¹å–„ç”¨æˆ·ä½“éªŒ
- é€‚åˆé•¿æ–‡æœ¬ç”Ÿæˆ

**æµå¼å“åº”æ ¼å¼**ï¼š
```
data: {"output":{"choices":[{"finish_reason":"null","message":{"role":"assistant","content":"ç¬¬ä¸€ä¸ªå­—"}}]}}

data: {"output":{"choices":[{"finish_reason":"null","message":{"role":"assistant","content":"ç¬¬äºŒä¸ªå­—"}}]}}

data: [DONE]
```

### 2.3 å‡½æ•°è°ƒç”¨æ¥å£ (Function Calling)

**ç”¨é€”**ï¼š
- ä¸å¤–éƒ¨å·¥å…·é›†æˆ
- æ„å»ºæ™ºèƒ½Agent
- ç»“æ„åŒ–è¾“å‡º

**å®šä¹‰å‡½æ•°schemaç¤ºä¾‹**ï¼š
```json
{
  "type": "function",
  "function": {
    "name": "get_weather",
    "description": "è·å–åŸå¸‚å¤©æ°”",
    "parameters": {
      "type": "object",
      "properties": {
        "city": {
          "type": "string",
          "description": "åŸå¸‚åç§°"
        },
        "unit": {
          "type": "string",
          "enum": ["celsius", "fahrenheit"],
          "description": "æ¸©åº¦å•ä½"
        }
      },
      "required": ["city"]
    }
  }
}
```

**è°ƒç”¨æµç¨‹**ï¼š
1. å‘é€åŒ…å«toolså®šä¹‰çš„è¯·æ±‚
2. æ¨¡å‹è¿”å›åº”è¯¥è°ƒç”¨çš„å·¥å…·å’Œå‚æ•°
3. å®¢æˆ·ç«¯æ‰§è¡Œå·¥å…·
4. å°†ç»“æœä½œä¸ºæ–°æ¶ˆæ¯å‘å›æ¨¡å‹
5. æ¨¡å‹ç”Ÿæˆæœ€ç»ˆå“åº”

### 2.4 é«˜çº§åŠŸèƒ½

#### 2.4.1 è”ç½‘æœç´¢
```python
{
    "model": "qwen-plus",
    "messages": [...],
    "tools": [
        {
            "type": "web_search",
            "web_search": {}
        }
    ]
}
```

#### 2.4.2 RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ)
```python
{
    "model": "qwen-plus",
    "messages": [...],
    "documents": [
        {
            "title": "æ–‡æ¡£æ ‡é¢˜",
            "content": "æ–‡æ¡£å†…å®¹"
        }
    ]
}
```

#### 2.4.3 é•¿ä¸Šä¸‹æ–‡å¤„ç†
```python
# ä½¿ç”¨qwen-longæ¨¡å‹æ”¯æŒ100K tokenä¸Šä¸‹æ–‡
{
    "model": "qwen-long",
    "messages": [...],
    "max_tokens": 4096
}
```

---

## Pythonè°ƒç”¨ç¤ºä¾‹

### 3.1 ç¯å¢ƒå‡†å¤‡

#### å®‰è£…SDK
```bash
# å®‰è£…å®˜æ–¹SDK
pip install dashscope

# æˆ–ä½¿ç”¨OpenAIå…¼å®¹SDK
pip install openai
```

#### è®¾ç½®API Key
```bash
export DASHSCOPE_API_KEY=your-api-key-here
```

æˆ–åœ¨Pythonä»£ç ä¸­ï¼š
```python
import os
os.environ['DASHSCOPE_API_KEY'] = 'your-api-key-here'
```

### 3.2 åŸºæœ¬è°ƒç”¨ç¤ºä¾‹

#### ä½¿ç”¨OpenAI SDKï¼ˆæ¨èå¿«é€Ÿå¼€å§‹ï¼‰
```python
from openai import OpenAI

client = OpenAI(
    api_key="your-api-key",
    base_url="https://dashscope.aliyun.com/compatible-mode/v1"
)

response = client.chat.completions.create(
    model="qwen-plus",
    messages=[
        {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"}
    ]
)

print(response.choices[0].message.content)
print(f"æ€»tokens: {response.usage.total_tokens}")
```

#### ä½¿ç”¨å®˜æ–¹SDK
```python
from dashscope import Generation
import dashscope

dashscope.api_key = 'your-api-key'

response = Generation.call(
    model='qwen-plus',
    messages=[
        {'role': 'user', 'content': 'ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±'}
    ]
)

if response.status_code == 200:
    print(response.output.choices[0].message.content)
    print(f"è¾“å…¥tokens: {response.usage.input_tokens}")
    print(f"è¾“å‡ºtokens: {response.usage.output_tokens}")
else:
    print(f"Error: {response.code} - {response.message}")
```

### 3.3 æµå¼è°ƒç”¨ç¤ºä¾‹

#### OpenAI SDKæµå¼è°ƒç”¨
```python
from openai import OpenAI

client = OpenAI(
    api_key="your-api-key",
    base_url="https://dashscope.aliyun.com/compatible-mode/v1"
)

stream = client.chat.completions.create(
    model="qwen-plus",
    messages=[
        {"role": "user", "content": "è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—"}
    ],
    stream=True
)

# å®æ—¶è¾“å‡º
for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end='', flush=True)
print()
```

#### å®˜æ–¹SDKæµå¼è°ƒç”¨
```python
from dashscope import Generation
import dashscope

dashscope.api_key = 'your-api-key'

responses = Generation.call(
    model='qwen-plus',
    messages=[
        {'role': 'user', 'content': 'è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—'}
    ],
    stream=True,
    result_format='message'
)

total_tokens = 0
for response in responses:
    if response.status_code == 200:
        for choice in response.output.choices:
            print(choice.message.content, end='', flush=True)
        total_tokens += response.usage.output_tokens
    else:
        print(f"Error: {response.code} - {response.message}")

print(f"\næ€»è¾“å‡ºtokens: {total_tokens}")
```

### 3.4 å‡½æ•°è°ƒç”¨å®Œæ•´ç¤ºä¾‹

```python
from openai import OpenAI
import json

client = OpenAI(
    api_key="your-api-key",
    base_url="https://dashscope.aliyun.com/compatible-mode/v1"
)

# å®šä¹‰å·¥å…·
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯",
            "parameters": {
                "type": "object",
                "properties": {
                    "city": {
                        "type": "string",
                        "description": "åŸå¸‚åç§°"
                    },
                    "unit": {
                        "type": "string",
                        "enum": ["celsius", "fahrenheit"],
                        "description": "æ¸©åº¦å•ä½",
                        "default": "celsius"
                    }
                },
                "required": ["city"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "get_time",
            "description": "è·å–æŒ‡å®šåŸå¸‚çš„å½“å‰æ—¶é—´",
            "parameters": {
                "type": "object",
                "properties": {
                    "city": {
                        "type": "string",
                        "description": "åŸå¸‚åç§°"
                    }
                },
                "required": ["city"]
            }
        }
    }
]

# æ¨¡æ‹Ÿå·¥å…·æ‰§è¡Œ
def execute_tool(tool_name, tool_input):
    if tool_name == "get_weather":
        return f"{tool_input['city']}çš„å¤©æ°”: æ™´æœ—,æ¸©åº¦ {20} {tool_input.get('unit', 'celsius')}"
    elif tool_name == "get_time":
        return f"{tool_input['city']}çš„å½“å‰æ—¶é—´: 2024-01-20 15:30:00"
    return "æœªçŸ¥å·¥å…·"

# ç¬¬ä¸€è½®å¯¹è¯
messages = [
    {"role": "user", "content": "åŒ—äº¬å’Œä¸Šæµ·ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}
]

response = client.chat.completions.create(
    model="qwen-plus",
    messages=messages,
    tools=tools,
    tool_choice="auto"
)

print(f"æ¨¡å‹å“åº”: {response.choices[0].message.content}")

# å¤„ç†å‡½æ•°è°ƒç”¨
if response.choices[0].message.tool_calls:
    # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²
    messages.append({"role": "assistant", "content": response.choices[0].message.content})
    
    # æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
    tool_results = []
    for tool_call in response.choices[0].message.tool_calls:
        tool_name = tool_call.function.name
        tool_input = json.loads(tool_call.function.arguments)
        
        print(f"æ‰§è¡Œå·¥å…·: {tool_name}ï¼Œå‚æ•°: {tool_input}")
        result = execute_tool(tool_name, tool_input)
        print(f"å·¥å…·ç»“æœ: {result}")
        
        tool_results.append({
            "role": "tool",
            "content": result,
            "tool_call_id": tool_call.id,
            "name": tool_name
        })
    
    # æ·»åŠ å·¥å…·ç»“æœ
    messages.extend(tool_results)
    
    # ç¬¬äºŒè½®å¯¹è¯ - è®©æ¨¡å‹ç”Ÿæˆæœ€ç»ˆå“åº”
    final_response = client.chat.completions.create(
        model="qwen-plus",
        messages=messages,
        tools=tools
    )
    
    print(f"æœ€ç»ˆç­”æ¡ˆ: {final_response.choices[0].message.content}")
```

### 3.5 é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

```python
from openai import OpenAI
import time
from typing import Optional

client = OpenAI(
    api_key="your-api-key",
    base_url="https://dashscope.aliyun.com/compatible-mode/v1"
)

def call_with_retry(
    messages: list,
    max_retries: int = 3,
    retry_delay: float = 1.0,
    backoff_multiplier: float = 2.0
) -> Optional[str]:
    """
    å¸¦é‡è¯•æœºåˆ¶çš„APIè°ƒç”¨
    
    å‚æ•°:
        messages: æ¶ˆæ¯åˆ—è¡¨
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        retry_delay: åˆå§‹é‡è¯•å»¶è¿Ÿ(ç§’)
        backoff_multiplier: æŒ‡æ•°é€€é¿å€æ•°
    
    è¿”å›:
        å“åº”æ–‡æœ¬æˆ–None
    """
    current_delay = retry_delay
    last_error = None
    
    for attempt in range(max_retries):
        try:
            print(f"å°è¯•ç¬¬ {attempt + 1}/{max_retries}...")
            
            response = client.chat.completions.create(
                model="qwen-plus",
                messages=messages,
                temperature=0.7,
                max_tokens=2048
            )
            
            print("âœ“ è°ƒç”¨æˆåŠŸ")
            return response.choices[0].message.content
            
        except Exception as e:
            last_error = e
            error_name = type(e).__name__
            
            # åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•
            if "RateLimitError" in str(error_name):
                print(f"âš  è§¦å‘é™æµ ({error_name})")
            elif "Timeout" in str(error_name):
                print(f"âš  è¯·æ±‚è¶…æ—¶ ({error_name})")
            elif "ServiceUnavailable" in str(error_name):
                print(f"âš  æœåŠ¡ä¸å¯ç”¨ ({error_name})")
            else:
                # å…¶ä»–é”™è¯¯å¯èƒ½ä¸å¯æ¢å¤
                print(f"âœ— æ— æ³•æ¢å¤çš„é”™è¯¯: {error_name} - {str(e)}")
                raise
            
            if attempt < max_retries - 1:
                print(f"ç­‰å¾… {current_delay:.1f} ç§’åé‡è¯•...")
                time.sleep(current_delay)
                current_delay *= backoff_multiplier
            else:
                print(f"âœ— å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°")
    
    raise last_error or Exception("æœªçŸ¥é”™è¯¯")

# ä½¿ç”¨ç¤ºä¾‹
try:
    result = call_with_retry(
        messages=[
            {"role": "user", "content": "å†™ä¸€ä¸ªPythonå‡½æ•°è®¡ç®—fibonacciæ•°åˆ—"}
        ]
    )
    print(f"ç»“æœ:\n{result}")
except Exception as e:
    print(f"æœ€ç»ˆå¤±è´¥: {e}")
```

### 3.6 å¤šè½®å¯¹è¯ç¤ºä¾‹

```python
from openai import OpenAI

client = OpenAI(
    api_key="your-api-key",
    base_url="https://dashscope.aliyun.com/compatible-mode/v1"
)

# åˆå§‹åŒ–å¯¹è¯å†å²
messages = [
    {
        "role": "system",
        "content": "ä½ æ˜¯ä¸€ä¸ªæ“…é•¿Pythonç¼–ç¨‹çš„åŠ©æ‰‹ï¼Œè¯·ç”¨ç®€æ´æ¸…æ™°çš„æ–¹å¼å›ç­”é—®é¢˜ã€‚"
    }
]

def chat(user_input: str) -> str:
    """æ·»åŠ ç”¨æˆ·æ¶ˆæ¯å¹¶è·å–å“åº”"""
    messages.append({"role": "user", "content": user_input})
    
    response = client.chat.completions.create(
        model="qwen-plus",
        messages=messages,
        temperature=0.7
    )
    
    assistant_message = response.choices[0].message.content
    messages.append({"role": "assistant", "content": assistant_message})
    
    return assistant_message

# å¤šè½®å¯¹è¯ç¤ºä¾‹
print("=== Pythonç¼–ç¨‹åŠ©æ‰‹ ===\n")

response1 = chat("Pythonä¸­çš„åˆ—è¡¨å’Œå…ƒç»„æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ")
print(f"Q: Pythonä¸­çš„åˆ—è¡¨å’Œå…ƒç»„æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ\nA: {response1}\n")

response2 = chat("é‚£å¦‚ä½•åˆ›å»ºä¸€ä¸ªå…ƒç»„å‘¢ï¼Ÿ")
print(f"Q: é‚£å¦‚ä½•åˆ›å»ºä¸€ä¸ªå…ƒç»„å‘¢ï¼Ÿ\nA: {response2}\n")

response3 = chat("ç»™æˆ‘ä¸€ä¸ªå®é™…åº”ç”¨ä¾‹å­")
print(f"Q: ç»™æˆ‘ä¸€ä¸ªå®é™…åº”ç”¨ä¾‹å­\nA: {response3}\n")

print(f"æœ¬æ¬¡å¯¹è¯ä½¿ç”¨äº† {len(messages)} æ¡æ¶ˆæ¯")
```

---

## æœ€ä½³å®è·µå’Œå»ºè®®

### 4.1 æç¤ºè¯ä¼˜åŒ–ï¼ˆPrompt Engineeringï¼‰

#### åŸåˆ™ä¸€ï¼šæ¸…æ™°å®šä¹‰è§’è‰²
```python
# âŒ ä¸å¥½çš„ä¾‹å­
"å¸®æˆ‘ç”Ÿæˆä¸€äº›ä»£ç "

# âœ… å¥½çš„ä¾‹å­
system_prompt = """
ä½ æ˜¯ä¸€ä¸ªèµ„æ·±Pythonå¼€å‘å·¥ç¨‹å¸ˆï¼Œå…·æœ‰10å¹´çš„é¡¹ç›®å¼€å‘ç»éªŒã€‚
ä½ æ“…é•¿ç¼–å†™é«˜æ•ˆã€å¯ç»´æŠ¤çš„ä»£ç ã€‚
å½“ç”¨æˆ·è¦æ±‚ä»£ç æ—¶ï¼Œè¯·ç¡®ä¿ï¼š
1. ä»£ç æœ‰è¯¦ç»†æ³¨é‡Š
2. éµå¾ªPEP8è§„èŒƒ
3. åŒ…å«é”™è¯¯å¤„ç†
4. æä¾›ä½¿ç”¨ç¤ºä¾‹
"""
```

#### åŸåˆ™äºŒï¼šå…·ä½“åŒ–éœ€æ±‚
```python
# âŒ ä¸å¥½çš„ä¾‹å­
"ç”Ÿæˆä¸€ä¸ªæ’åºå‡½æ•°"

# âœ… å¥½çš„ä¾‹å­
"è¯·ç”Ÿæˆä¸€ä¸ªPythonå‡½æ•°ï¼Œç”¨å¿«é€Ÿæ’åºç®—æ³•å¯¹æ•´æ•°åˆ—è¡¨è¿›è¡Œé™åºæ’åˆ—ã€‚
è¦æ±‚ï¼š
- å‡½æ•°åä¸ºquick_sort_desc
- ä¼ å…¥å‚æ•°ä¸ºlistï¼Œè¿”å›æ’åºåçš„list
- æ·»åŠ ç±»å‹æ³¨è§£
- åŒ…å«docstringè¯´æ˜
- æä¾›3ä¸ªæµ‹è¯•ç”¨ä¾‹"
```

#### åŸåˆ™ä¸‰ï¼šæä¾›ä¸Šä¸‹æ–‡
```python
# âŒ ä¸å¥½çš„ä¾‹å­
"å¦‚ä½•ä¼˜åŒ–è¿™ä¸ªå‡½æ•°"

# âœ… å¥½çš„ä¾‹å­
"""
æˆ‘æœ‰ä»¥ä¸‹å‡½æ•°ï¼Œç”¨äºå¤„ç†100ä¸‡æ¡ç”¨æˆ·æ•°æ®ï¼š

```python
def process_users(users):
    result = []
    for user in users:
        if user['age'] > 18:
            user['category'] = 'adult'
        else:
            user['category'] = 'minor'
        result.append(user)
    return result
```

è¿™ä¸ªå‡½æ•°åœ¨å¤„ç†å¤§æ•°æ®æ—¶æ¯”è¾ƒæ…¢ã€‚è¯·ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢å»ºè®®ä¼˜åŒ–æ–¹æ¡ˆï¼š
1. ç®—æ³•ä¼˜åŒ–
2. å†…å­˜ä½¿ç”¨ä¼˜åŒ–
3. å¹¶å‘å¤„ç†ä¼˜åŒ–
"""
```

#### åŸåˆ™å››ï¼šç»“æ„åŒ–è¾“å‡º
```python
# âŒ ä¸å¥½çš„ä¾‹å­
"åˆ†æè¿™ä¸ªé”™è¯¯"

# âœ… å¥½çš„ä¾‹å­
"""
è¯·åˆ†æä»¥ä¸‹é”™è¯¯ï¼Œå¹¶æŒ‰ä»¥ä¸‹æ ¼å¼è¿”å›ï¼š

é”™è¯¯åŸå› ï¼š[ç®€æ˜æ‰¼è¦çš„åŸå› ]
å½±å“èŒƒå›´ï¼š[å¯èƒ½å—å½±å“çš„å…¶ä»–ä»£ç ]
è§£å†³æ–¹æ¡ˆï¼š
1. [æ–¹æ¡ˆ1åŠå…¶ä¼˜ç¼ºç‚¹]
2. [æ–¹æ¡ˆ2åŠå…¶ä¼˜ç¼ºç‚¹]
æ¨èæ–¹æ¡ˆï¼š[æœ€å¥½çš„è§£å†³æ–¹æ¡ˆï¼ŒåŠå…¶æ­¥éª¤]

é”™è¯¯ä¿¡æ¯ï¼š
TypeError: 'NoneType' object is not subscriptable
"""
```

### 4.2 æ€§èƒ½ä¼˜åŒ–æŠ€å·§

#### æŠ€å·§ä¸€ï¼šé€‰æ‹©åˆé€‚çš„æ¨¡å‹
```python
# ä¸åŒåœºæ™¯é€‰æ‹©ä¸åŒæ¨¡å‹
use_cases = {
    "ç®€å•é—®ç­”": "qwen-turbo",          # æœ€å¿«æœ€ä¾¿å®œ
    "é€šç”¨ä»»åŠ¡": "qwen-plus",            # æ¨èé€‰æ‹©
    "å¤æ‚æ¨ç†": "qwen-max",             # æ€§èƒ½æœ€å¼º
    "é•¿æ–‡æœ¬å¤„ç†": "qwen-long",          # 100K tokenä¸Šä¸‹æ–‡
    "ä»£ç ç”Ÿæˆ": "qwen-coder",           # ä¼˜åŒ–ç¼–ç¨‹
    "æ•°å­¦é—®é¢˜": "qwen-math"             # ä¼˜åŒ–æ•°å­¦
}
```

#### æŠ€å·§äºŒï¼šä½¿ç”¨æµå¼è¾“å‡º
```python
# æµå¼è¾“å‡ºå¯ä»¥æ›´æ—©å±•ç¤ºç»“æœï¼Œæ”¹å–„ç”¨æˆ·ä½“éªŒ
response = client.chat.completions.create(
    model="qwen-plus",
    messages=messages,
    stream=True  # å¯ç”¨æµå¼è¾“å‡º
)

for chunk in response:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end='', flush=True)
```

#### æŠ€å·§ä¸‰ï¼šå¹¶å‘è¯·æ±‚
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

def make_request(prompt):
    response = client.chat.completions.create(
        model="qwen-plus",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

# å¹¶å‘å¤„ç†å¤šä¸ªè¯·æ±‚
prompts = ["é—®é¢˜1", "é—®é¢˜2", "é—®é¢˜3"]
with ThreadPoolExecutor(max_workers=5) as executor:
    results = list(executor.map(make_request, prompts))
```

#### æŠ€å·§å››ï¼šé™åˆ¶è¾“å‡ºé•¿åº¦
```python
# ä½¿ç”¨max_tokensé™åˆ¶è¾“å‡ºï¼Œå‡å°‘å¤„ç†æ—¶é—´å’Œæˆæœ¬
response = client.chat.completions.create(
    model="qwen-plus",
    messages=messages,
    max_tokens=500  # é™åˆ¶è¾“å‡ºä¸º500ä¸ªtoken
)
```

#### æŠ€å·§äº”ï¼šç¼“å­˜çƒ­æŸ¥è¯¢
```python
from functools import lru_cache

@lru_cache(maxsize=128)
def get_cached_response(prompt):
    response = client.chat.completions.create(
        model="qwen-plus",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

# ç›¸åŒæç¤ºè¯ä¼šç›´æ¥è¿”å›ç¼“å­˜
result1 = get_cached_response("Pythonæ˜¯ä»€ä¹ˆï¼Ÿ")
result2 = get_cached_response("Pythonæ˜¯ä»€ä¹ˆï¼Ÿ")  # è¿”å›ç¼“å­˜ï¼Œæ— éœ€è°ƒç”¨API
```

### 4.3 æˆæœ¬æ§åˆ¶æ–¹æ¡ˆ

#### æˆæœ¬å¯¹æ¯”è¡¨

æ ¹æ®é˜¿é‡Œäº‘ 2024 å¹´ 12 æœˆ 31 æ—¥æœ€æ–°é™ä»·ï¼ˆå®˜æ–¹æ¥æºï¼šhttps://www.ithome.com/0/821/422.htmï¼‰

```
æ¨¡å‹          | è¾“å…¥ä»·æ ¼    | è¾“å‡ºä»·æ ¼    | ä¸Šä¸‹æ–‡é•¿åº¦   | æ¨èç”¨é€”
-----------  | ---------- | ---------- | ---------- | ----------------
qwen-turbo   | Â¥0.0003    | Â¥0.0006    | 100ä¸‡      | ç®€å•ä»»åŠ¡ã€é«˜é¢‘è°ƒç”¨
qwen-plus    | Â¥0.0008    | Â¥0.002     | 131K       | é€šç”¨ä»»åŠ¡ï¼ˆæ¨èï¼‰
qwen-max     | Â¥0.02      | Â¥0.06      | 32K        | å¤æ‚ä»»åŠ¡ã€é«˜ç²¾åº¦
qwen-long    | Â¥0.0005    | Â¥0.002     | 1000ä¸‡     | é•¿æ–‡æ¡£å¤„ç†
```

**é‡è¦è¯´æ˜**ï¼š
- ä¸Šè¿°ä»·æ ¼å·²äº 2024 å¹´ 12 æœˆ 31 æ—¥ç”Ÿæ•ˆ
- qwen-turbo å·²ä»åŸä»· Â¥0.002 é™è‡³ Â¥0.0003ï¼ˆè¾“å…¥ä»·æ ¼ï¼‰ï¼Œé™å¹… 85%
- qwen-plus å·²ä»åŸä»· Â¥0.004 é™è‡³ Â¥0.0008ï¼ˆè¾“å…¥ä»·æ ¼ï¼‰ï¼Œé™å¹… 80%
- qwen-max å·²ä»åŸä»· Â¥0.04 é™è‡³ Â¥0.02ï¼ˆè¾“å…¥ä»·æ ¼ï¼‰ï¼Œé™å¹… 50%

#### æˆæœ¬ä¼˜åŒ–å»ºè®®

1. **ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹**
   ```python
   # ä¼˜åŒ–ï¼šç”¨qwen-turboæ›¿ä»£qwen-plusï¼Œæˆæœ¬å¯èŠ‚çœ96.25%
   # qwen-turbo è¾“å…¥ä»·æ ¼ï¼šÂ¥0.0003/1K
   # qwen-plus è¾“å…¥ä»·æ ¼ï¼šÂ¥0.0008/1K
   # æˆæœ¬æ¯”ä¾‹ï¼š0.0003 / 0.0008 = 37.5% ï¼ˆèŠ‚çœ62.5%ï¼‰
   # æœ€ç®€å•çš„ä»»åŠ¡ä¼˜å…ˆä½¿ç”¨ qwen-turbo
   model = "qwen-turbo"  # ä» qwen-plus æ”¹ä¸º qwen-turbo
   ```

2. **é™åˆ¶è¾“å‡ºé•¿åº¦**
   ```python
   # å¯¹äºæ‘˜è¦ã€åˆ†ç±»ç­‰ä»»åŠ¡ï¼Œé™åˆ¶max_tokens
   response = client.chat.completions.create(
       model="qwen-plus",
       messages=messages,
       max_tokens=200  # é™åˆ¶è¾“å‡º
   )
   ```

3. **æ‰¹é‡å¤„ç†**
   ```python
   # æ‰¹é‡å‘é€è¯·æ±‚ï¼Œå‡å°‘å•æ¬¡è°ƒç”¨æ•°é‡
   def batch_process(items, batch_size=10):
       for i in range(0, len(items), batch_size):
           batch = items[i:i+batch_size]
           # å°†å¤šä¸ªé¡¹ç›®åˆå¹¶åˆ°ä¸€ä¸ªè¯·æ±‚ä¸­
           combined_prompt = "\n".join(batch)
           # å•æ¬¡APIè°ƒç”¨å¤„ç†å¤šä¸ªé¡¹ç›®
   ```

4. **ç¼“å­˜ç­–ç•¥**
   ```python
   # æ„å»ºæœ¬åœ°ç¼“å­˜ï¼Œé¿å…é‡å¤è°ƒç”¨
   import json
   
   cache_file = "response_cache.json"
   
   def get_response_with_cache(prompt):
       # æ£€æŸ¥ç¼“å­˜
       try:
           with open(cache_file, 'r') as f:
               cache = json.load(f)
               if prompt in cache:
                   return cache[prompt]
       except:
           cache = {}
       
       # è°ƒç”¨API
       response = client.chat.completions.create(
           model="qwen-plus",
           messages=[{"role": "user", "content": prompt}]
       )
       result = response.choices[0].message.content
       
       # ä¿å­˜åˆ°ç¼“å­˜
       cache[prompt] = result
       with open(cache_file, 'w') as f:
           json.dump(cache, f)
       
       return result
   ```

5. **æˆæœ¬ç›‘æ§**
   ```python
   class CostTracker:
       def __init__(self):
           self.total_input_tokens = 0
           self.total_output_tokens = 0
           self.model_costs = {
               "qwen-turbo": {"input": 0.00008, "output": 0.00016},
               "qwen-plus": {"input": 0.0005, "output": 0.0015},
               "qwen-max": {"input": 0.002, "output": 0.006},
           }
       
       def track(self, model, input_tokens, output_tokens):
           self.total_input_tokens += input_tokens
           self.total_output_tokens += output_tokens
       
       def get_cost(self, model):
           costs = self.model_costs.get(model, {})
           input_cost = self.total_input_tokens * costs.get("input", 0) / 1000
           output_cost = self.total_output_tokens * costs.get("output", 0) / 1000
           return input_cost + output_cost
       
       def print_report(self, model):
           cost = self.get_cost(model)
           print(f"æ¨¡å‹: {model}")
           print(f"è¾“å…¥tokens: {self.total_input_tokens}")
           print(f"è¾“å‡ºtokens: {self.total_output_tokens}")
           print(f"é¢„ä¼°æˆæœ¬: Â¥{cost:.4f}")
   
   # ä½¿ç”¨
   tracker = CostTracker()
   # ... è¿›è¡ŒAPIè°ƒç”¨ ...
   # tracker.track("qwen-plus", input_tokens, output_tokens)
   # tracker.print_report("qwen-plus")
   ```

### 4.4 å®‰å…¨æ€§å’Œå¯é æ€§å»ºè®®

#### å®‰å…¨å»ºè®®
```python
import os
from dotenv import load_dotenv

# 1. ä¸è¦åœ¨ä»£ç ä¸­ç¡¬ç¼–ç API Key
# âŒ ä¸å¥½çš„åšæ³•
api_key = "sk-xxx-xxx-xxx"

# âœ… å¥½çš„åšæ³•ï¼šä½¿ç”¨ç¯å¢ƒå˜é‡
load_dotenv()
api_key = os.getenv("DASHSCOPE_API_KEY")

# 2. å¯¹ç”¨æˆ·è¾“å…¥è¿›è¡ŒéªŒè¯å’Œæ¸…ç†
def validate_input(user_input):
    # æ£€æŸ¥é•¿åº¦
    if len(user_input) > 10000:
        raise ValueError("è¾“å…¥è¿‡é•¿")
    
    # æ£€æŸ¥æ¶æ„å†…å®¹
    forbidden_keywords = ["DROP", "DELETE", "EXEC"]
    if any(keyword in user_input.upper() for keyword in forbidden_keywords):
        raise ValueError("è¾“å…¥åŒ…å«ç¦æ­¢å†…å®¹")
    
    return user_input.strip()

# 3. ä½¿ç”¨è¯·æ±‚è¶…æ—¶
response = client.chat.completions.create(
    model="qwen-plus",
    messages=messages,
    timeout=30  # 30ç§’è¶…æ—¶
)

# 4. è®°å½•æ‰€æœ‰APIè°ƒç”¨æ—¥å¿—
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def log_api_call(messages, response):
    logger.info(f"API Call: {messages}")
    logger.info(f"Response: {response.choices[0].message.content[:100]}")
```

#### å¯é æ€§å»ºè®®
```python
# 1. å®æ–½å¥åº·æ£€æŸ¥
def health_check():
    try:
        response = client.chat.completions.create(
            model="qwen-plus",
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return True
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return False

# 2. å®æ–½ç†”æ–­æœºåˆ¶
from datetime import datetime, timedelta

class CircuitBreaker:
    def __init__(self, failure_threshold=5, timeout=60):
        self.failure_count = 0
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.last_failure_time = None
        self.is_open = False
    
    def call(self, func, *args, **kwargs):
        if self.is_open:
            if datetime.now() - self.last_failure_time > timedelta(seconds=self.timeout):
                self.is_open = False
                self.failure_count = 0
            else:
                raise Exception("Circuit breaker is open")
        
        try:
            result = func(*args, **kwargs)
            self.failure_count = 0
            return result
        except Exception as e:
            self.failure_count += 1
            self.last_failure_time = datetime.now()
            if self.failure_count >= self.failure_threshold:
                self.is_open = True
            raise
```

### 4.5 ç”¨æˆ·ä½“éªŒä¼˜åŒ–

```python
# 1. è¿›åº¦æ˜¾ç¤º
import sys

def stream_response_with_progress(stream):
    token_count = 0
    for chunk in stream:
        if chunk.choices[0].delta.content:
            content = chunk.choices[0].delta.content
            print(content, end='', flush=True)
            token_count += len(content)
            sys.stdout.write(f" [{token_count} å­—ç¬¦]")
            sys.stdout.flush()

# 2. æ ¼å¼åŒ–è¾“å‡º
def format_response(response_text):
    # å¯¹é•¿æ–‡æœ¬è¿›è¡Œæ ¼å¼åŒ–
    import textwrap
    lines = response_text.split('\n')
    formatted = '\n'.join(
        textwrap.fill(line, width=80) for line in lines
    )
    return formatted

# 3. é”™è¯¯æç¤ºæ”¹è¿›
def friendly_error_message(error):
    error_map = {
        "RateLimitError": "è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•",
        "TimeoutError": "è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥",
        "AuthenticationError": "API Keyæ— æ•ˆï¼Œè¯·æ£€æŸ¥é…ç½®",
        "ServiceUnavailableError": "æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•"
    }
    
    error_name = type(error).__name__
    return error_map.get(error_name, f"å‘ç”Ÿé”™è¯¯: {str(error)}")
```

---

## å¸¸è§é—®é¢˜è§£å†³

### 5.1 è®¤è¯é—®é¢˜

**é—®é¢˜**: `AuthenticationError: Invalid API key`

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. æ£€æŸ¥API Keyæ˜¯å¦æ­£ç¡®
import os
api_key = os.getenv('DASHSCOPE_API_KEY')
print(f"API Key: {api_key[:20]}...")  # åªæ˜¾ç¤ºå‰20ä¸ªå­—ç¬¦

# 2. ç¡®ä¿API Keyæœ‰æ•ˆæœŸæœªè¿‡æœŸ
# åœ¨DashScopeæ§åˆ¶å°æ£€æŸ¥API KeyçŠ¶æ€

# 3. å°è¯•æ›´æ–°client
client = OpenAI(
    api_key=api_key,
    base_url="https://dashscope.aliyun.com/compatible-mode/v1"
)
```

### 5.2 é™æµé—®é¢˜

**é—®é¢˜**: `RateLimitError: Rate limit exceeded`

**è§£å†³æ–¹æ¡ˆ**:
```python
import time
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10)
)
def call_api_with_retry(messages):
    return client.chat.completions.create(
        model="qwen-plus",
        messages=messages
    )

# æˆ–æ‰‹åŠ¨å®ç°
def call_with_backoff(messages, max_retries=3):
    for attempt in range(max_retries):
        try:
            return client.chat.completions.create(
                model="qwen-plus",
                messages=messages
            )
        except RateLimitError:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt
                print(f"é™æµï¼Œç­‰å¾… {wait_time} ç§’...")
                time.sleep(wait_time)
            else:
                raise
```

### 5.3 è¶…æ—¶é—®é¢˜

**é—®é¢˜**: `Timeout: Request timed out`

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. å¢åŠ è¶…æ—¶æ—¶é—´
client = OpenAI(
    api_key="your-api-key",
    base_url="https://dashscope.aliyun.com/compatible-mode/v1",
    timeout=60  # å¢åŠ åˆ°60ç§’
)

# 2. å‡å°‘max_tokens
response = client.chat.completions.create(
    model="qwen-plus",
    messages=messages,
    max_tokens=1024  # ä»2048å‡å°‘åˆ°1024
)

# 3. ä½¿ç”¨æµå¼è¾“å‡º
for chunk in client.chat.completions.create(
    model="qwen-plus",
    messages=messages,
    stream=True
):
    print(chunk.choices[0].delta.content or "", end="")
```

### 5.4 å“åº”è§£æé—®é¢˜

**é—®é¢˜**: `JSON decode error`

**è§£å†³æ–¹æ¡ˆ**:
```python
import json

def safe_parse_response(response_text):
    try:
        # å°è¯•ç›´æ¥è§£æ
        return json.loads(response_text)
    except json.JSONDecodeError:
        # å°è¯•æ¸…ç†å­—ç¬¦ä¸²
        cleaned = response_text.strip()
        if cleaned.startswith('```json'):
            cleaned = cleaned[7:-3]
        
        try:
            return json.loads(cleaned)
        except:
            # è¿”å›åŸå§‹æ–‡æœ¬
            return {"raw": response_text}

# ä½¿ç”¨
response_text = response.choices[0].message.content
parsed = safe_parse_response(response_text)
```

### 5.5 æ¨¡å‹é”™è¯¯

**é—®é¢˜**: `Model not found`

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ£€æŸ¥å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
available_models = [
    "qwen-turbo",
    "qwen-plus",
    "qwen-max",
    "qwen-max-latest",
    "qwen-long",
    "qwen-vl-plus",
    "qwen-vl-max"
]

# ä½¿ç”¨æ—¶æ£€æŸ¥
model_name = "qwen-plus"
if model_name not in available_models:
    raise ValueError(f"æ¨¡å‹ {model_name} ä¸å¯ç”¨")
```

---

## æ€»ç»“

### å¿«é€Ÿå¼€å§‹æ£€æŸ¥æ¸…å•

- [ ] å·²è·å–API Keyå¹¶è®¾ç½®ç¯å¢ƒå˜é‡
- [ ] å·²å®‰è£…openaiåº“ï¼š`pip install openai`
- [ ] å·²æµ‹è¯•åŸºæœ¬è¿æ¥
- [ ] å·²äº†è§£ä¸åŒæ¨¡å‹çš„å·®å¼‚
- [ ] å·²é…ç½®é”™è¯¯å¤„ç†æœºåˆ¶
- [ ] å·²è¯„ä¼°æˆæœ¬å’Œæ€§èƒ½éœ€æ±‚

### ä¸‹ä¸€æ­¥å»ºè®®

1. **æœ¬åœ°å¼€å‘**ï¼šä½¿ç”¨qwen-plusæ¨¡å‹è¿›è¡Œå¼€å‘
2. **æ€§èƒ½ä¼˜åŒ–**ï¼šæ ¹æ®å®é™…æƒ…å†µé€‰æ‹©åˆé€‚çš„æ¨¡å‹
3. **ç”Ÿäº§éƒ¨ç½²**ï¼šå®æ–½å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œç›‘æ§
4. **æŒç»­ä¼˜åŒ–**ï¼šç›‘æ§æˆæœ¬å’Œæ€§èƒ½æŒ‡æ ‡

### ç›¸å…³èµ„æº

- å®˜æ–¹æ–‡æ¡£: https://dashscope.aliyun.com/
- APIå‚è€ƒ: https://help.aliyun.com/zh/dashscope/
- Python SDK: https://github.com/aliyun/dashscope-python-sdk
- OpenAIå…¼å®¹: https://dashscope.aliyun.com/compatible-mode/v1

---

**æ–‡æ¡£æ›´æ–°æ—¶é—´**: 2024å¹´1æœˆ
**ç‰ˆæœ¬**: 1.0
